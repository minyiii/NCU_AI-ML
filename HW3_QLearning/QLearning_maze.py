# -*- coding: utf-8 -*-
"""HW3_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TERybE86wVZOA_JublkQBXVD4mkt_pJx
"""

#-------import------------
import numpy as np
import pandas as pd
import time

np.random.seed(6)

#-------global variable---------
# N_STATES = 6   # 有多少種states(距離寶藏有多少步)
ACTIONS = ['left', 'right', 'up', 'down']     # 有四種可以做的動作
EPSILON = 0.9   # epsilon greedy，代表以「獎勵多寡」來選action的機率；剩下機率(1-EPSILON)會隨機選
ALPHA = 0.08     # learning rate
GAMMA = 0.9    # discount factor 未來獎勵的衰減值
MAX_EPISODES = 300   # maximum episodes 玩?回合
FRESH_TIME = 0.01 # 0.3   # fresh time for one move 走一步花的時間

Q_TABLE =  pd.DataFrame()

ENV_COL = 30 # 環境的總column數
ENV_ROW = 40 # 環境的總row數

# 各action移動時的座標變化
dirs = {'left': np.array([0, -1]), 'right': np.array([0, 1]), 
        'up': np.array([-1,0]), 'down': np.array([1,0])}

# string轉成int array
def str_to_arr(text):
    # 先以','切開字元，再用map把每個元素轉成int，把map轉為list再變成array
    arr = np.array(list(map(int, text.split(","))))
    # print(arr[0]+arr[1])
    return arr

# int array轉成string
def arr_to_str(arr):
    text=""
    text = str(arr[0])+','+ str(arr[1])
    return text

#--------建立Q table----------
def build_q_table(actions): 
    # 以0初始化整張Q table，col：action、row：state
    table = pd.DataFrame(columns=actions) # 因為不知道有多少state所以就不初始化為0
    # print(table)
    return table

# 看目前state(也就是座標)是否存在於Q table，若沒有會把當下state加進Q table中
def check_state_exist(state):
    global Q_TABLE
    if state not in Q_TABLE.index:
        Q_TABLE = Q_TABLE.append(
            pd.Series(
              [0]*len(ACTIONS), # 各個action在此狀態的初始化Q值皆為0
              # index = [state],
              index = Q_TABLE.columns,
              name = state,
              ).astype('float')
        )
        # print(Q_TABLE)
    # return Q_TABLE

#--------choose action的功能-----------
# 依目前state去對照目前Q table，取出此狀態下執行各action的獎勵
# 有EPSILON機率會用到剛取出的那row來挑最佳獎勵(獎勵max)的action，剩下機率(1-EPSILON)為隨機選擇
# 另外，一開始也只能用random來選比較合適(因為一開始整張Q table都是0，比獎勵大小也沒用)
def choose_action(state):
    check_state_exist(state)
    state_actions = Q_TABLE.loc[state] #取state這一行的對應資料

    #act non-greedy or state-action have no value
    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):
        action_name = np.random.choice(ACTIONS)
    else:   # act greedy
        # 同state可能有多個具備相同Q值的action，若不打亂的話，每次都只會選前面那個action
        state_actions = state_actions.reindex(np.random.permutation(state_actions.index))
        action_name = state_actions.idxmax() # 選該row(state)中比較大的col(action)，return它的col name
    return action_name


#--------建立環境對我們行為的feedback---------
# 若於狀態S中做動作A，狀態會變為S_，並取得獎勵R
def get_env_feedback(S, A):
    S = str_to_arr(S) # 要做矩陣相加所以轉成array
    new_S = S + dirs[A] # 照這action會到達的座標

    if (new_S[0] < 0) or (new_S[0]>ENV_ROW-1) or (new_S[1] < 0) or (new_S[1]>ENV_COL-1): # 新座標超過範圍
        S_ = arr_to_str(S) # 下個state依舊是舊座標，原地不動；要傳回去當index，所以轉回str
        R = -1 # 下次不要再撞牆了 
    elif new_S[0]==ENV_ROW-1 and new_S[1]==ENV_COL-1: # 找到寶藏
        S_ = 'terminal'
        R = 5
    else: # 一般狀況，沒找到但也沒超過範圍
        S_ = arr_to_str(new_S) # 要傳回去當index，所以轉回str
        # R = 0
        if A == 'right' or A =='down':
            R = 1
        else:
            R = 0
    return S_, R

def draw_maze(S):
    S = str_to_arr(S) # 轉成array
    env_list = ['-']*(ENV_COL)
    
    interaction = ''.join(env_list) # env_list每個元素間都間隔開
    for i in range(ENV_ROW-1):
        if(i==S[0]):
            env_list[S[1]]='o'
            interaction = ''.join(env_list) # env_list每個元素間都間隔開
            # print(i)
            print('\r{}'.format(interaction)) # 印出目前的樣子
            env_list[S[1]]='-'
            interaction = ''.join(env_list) # env_list每個元素間都間隔開
        else:
            # print(i)
            print('\r{}'.format(interaction)) # 印出目前的樣子

    if S[0]==ENV_ROW-1:
        env_list[S[1]]='o'
    env_list[ENV_COL-1]='T'
    interaction = ''.join(env_list) # env_list每個元素間都間隔開
    print('\r{}'.format(interaction), end='') # 印出目前的樣子


#-----------更新環境--------------
def update_env(S, episode, step_counter):
    print('episode: ', episode+1, ', step_counter: ', step_counter, '\n')
    if S == 'terminal': 
        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter) # 回應/因episode是由0開始，所以印出時要先+1
        print('\r{}'.format(interaction)) 
        time.sleep(2)

        print('\n\n')                  
        # for i in range(ENV_ROW-1):
        #     print('\r                                ') # 清空，\r代表回到本行行首
    else:
        draw_maze(S)
        print('\n\n')
        time.sleep(FRESH_TIME)

#----------建立reinforcement learning-----------
def rl():
    global Q_TABLE
    Q_TABLE = build_q_table(ACTIONS) #建立 Q table
    for episode in range(MAX_EPISODES): # 從第一個回合玩到最後一個回合
        step_counter = 0
        S = '0,0' # 初始情況，探索者放到左上角，即座標(0,0)
        is_terminated = False 
        update_env(S, episode, step_counter) #更新環境
        while not is_terminated: # 回合沒有結束
            A = choose_action(S) # 選擇action
            S_, R = get_env_feedback(S, A)  # 透過此action得到下個state和這次「確實」得到的獎勵
            check_state_exist(S_)
            q_predict = Q_TABLE.loc[S, A] # 估計值，當初看Q table上所寫的「預估」獎勵

            if S_ != 'terminal': # 回合還沒結束
                q_target = R + GAMMA * Q_TABLE.loc[S_, :].max()   # 真實值 
            else:
                q_target = R # 真實值
                is_terminated = True    # 結束這一回合；while結束，繼續下個for(回合)
                
            Q_TABLE.loc[S, A] += ALPHA * (q_target - q_predict)  # 更新Q table內容，加上(ALPHA*誤差)
            S = S_  # move to next state

            # 前往下個state
            update_env(S, episode, step_counter+1)
            step_counter += 1 # 紀錄步數
    return Q_TABLE

if __name__ == "__main__":
    Q_TABLE = rl() # 最終學習好的Q table
    # rl() # 最終學習好的Q table
    print('\r\nQ-table:\n')
    print(Q_TABLE)

print(Q_TABLE['up'].dtype)


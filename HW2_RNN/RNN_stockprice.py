# -*- coding: utf-8 -*-
"""0602_HW2_copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15nzy59k5RrDPzBU8P_MHLndCjFXV1v9A
"""

# 開檔案
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow.compat.v1 as tf
from google.colab import files
tf.disable_v2_behavior()

files.upload()

# 讀檔
# df1 = pd.read_csv('8046_2010_2019.csv')  #讀入南亞科股票數據
# df2 = pd.read_csv('3037_2010_2019_csv.csv')  #讀入欣興電子股票數據

f1=open(r'C:\Users\a9272\OneDrive\桌面\電商作業2\8046_2010_2019.csv')  
f2=open(r'C:\Users\a9272\OneDrive\桌面\電商作業2\3037_2010_2019_csv.csv')
df1=pd.read_csv(f1) # 讀入南亞科股票數據
df2=pd.read_csv(f2) # 讀入南亞科股票數據

# input_size為5
data = pd.DataFrame()
data=df1.iloc[:, 4:8]   # 取column 4~7
# data['8046 close'] = df1['close']
data['3037 close'] = df2['close']

data=data.dropna(axis=0,how='all')  #删除表中全部為NaN的row

print(data) # DataFrame，[2372 rows x 5 columns]
print(len(data)) # 2032

# print(data.open) # 一維陣列(Series) (2372, )；array([123. , 124.5, 131. , ...,  42.2])
# print(data.open.values.shape) # 一維陣列(2372,)；array([123. , 124.5, 131. , ...,  42.2])
# print(data.open.values.reshape(-1,1)) # 二維陣列(2372, 1)；[[123. ][124.5][131. ]...[ 42.2]]
# print(data.columns) # Index(['open', 'high', 'low', 'close', '3037 close'], dtype='object')
# print(data.columns[0]) # open
# print(len(data.columns)) # 5

# data[data.columns[0]]
# data.iloc[:,0]

# ------------處理data相關------------
# 標準化
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

def normalize_data(df):
    global scaler
    # 跑過所有column
    # for i in range(len(df.columns)):
    #   df.iloc[:,i] = scaler.fit_transform(df.iloc[i].values.reshape(-1,1))

    df['open'] = scaler.fit_transform(df.open.values.reshape(-1,1))
    df['high'] = scaler.fit_transform(df.high.values.reshape(-1,1))
    df['low'] = scaler.fit_transform(df.low.values.reshape(-1,1))
    df['close'] = scaler.fit_transform(df['close'].values.reshape(-1,1))
    close_scaler = scaler
    df['3037 close'] = scaler.fit_transform(df['3037 close'].values.reshape(-1,1))
    
    return df, close_scaler
  
# 標準化
# norm_data, close_scaler = normalize_data(data)
# print(norm_data)

# (1)標準化
# arr = norm_data.values # df轉arr：df.values
# (2)不標準化
arr = data.values # df轉arr：df.values
print(len(arr)) # 2372
print(arr.shape) # (2372, 5)
print(arr[0:3]) 
print(arr[0:3].shape) # (3, 5)

def load_data(df, time_step):
    data_raw = np.array(df) # DataFrame轉array
    data = []
    
    # create all possible sequences of length seq_len(time_step+1)
    for i in range(len(data_raw)-time_step):
        data.append(data_raw[i: i+time_step+1])

    data = np.array(data)
    test_size = 30 + time_step # 最後30筆中的第一筆需要他前面time_step筆來做預測
    train_size = data.shape[0] - test_size
    
    x_train = data[:train_size,:-1,:]
    # y_train = data[:train_size,-1,:]
    y_train = data[:train_size,-1,3:4]
    
    x_test = data[train_size:,:-1,:]
    # y_test = data[train_size:,-1,:] # [ 哪幾組, 一組有?個time_step, 每個col(attribute)]
    y_test = data[train_size:,-1,3:4] # [ 哪幾組, 一組有?個time_step, 每個col(attribute)]
    
    return [x_train, y_train, x_test, y_test]

# 定義常數
time_step = 20 # 看前?天來預測 # n_steps
input_size = 5 # input layer維度 # n_input
rnn_unit = 40 # # n_neurons
# output_size = 5 # output layer維度 # n_outputs
output_size = 1 # output layer維度 # n_outputs
lr = 0.0006 # 學習率
batch_size = 60 # 每batch訓練多少資料
layers = 3 # cell有幾layer
epochs = 1000

# 取得training set、test set
# (1) 標準化
# x_train, y_train, x_test, y_test = load_data(norm_data, time_step)
# (2) 不標準化
x_train, y_train, x_test, y_test = load_data(data, time_step)

train_size = x_train.shape[0]
test_size = x_test.shape[0]

print(x_train.shape) # (2312, 15, 5)
print(y_train.shape) # (2312, 5)
print(x_test.shape) # (45, 15, 5)
print(y_test.shape) # (45, 5)

# print(y_test)

# # len(data)
# t=[]
# t.append(1)
# t.append(3)
# t.append(2)
# print(t)

# # x_train.shape[0]
# perm_array  = np.arange(x_train.shape[0])
# np.random.shuffle(perm_array)
# perm_array

# print(perm_array[0:20])

# print(x_train[[94, 161]])
# print(x_train[perm_array[0:20]])

index_in_epoch = 0 # 指下個batch中取的資料裡的第一個index
# 用這array產生的亂數代表train data中的要取的資料index的順序，每次都取batch_size個，time_step天的input_size個值
perm_array  = np.arange(x_train.shape[0])
np.random.shuffle(perm_array)

def get_next_batch(batch_size):
    # 這三個變數需為global，如果是local用遠都改不到他的值
    global index_in_epoch, x_train, perm_array
    start = index_in_epoch
    index_in_epoch += batch_size
    
    # 這個epoch已全部都跑過一遍，再把這三個global variable初始化
    if index_in_epoch > x_train.shape[0]:
        np.random.shuffle(perm_array) # shuffle permutation array
        start = 0 # start next epoch
        index_in_epoch = batch_size
        
    end = index_in_epoch
    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]

# print(x_train[perm_array[0:batch_size]].shape) # (60, 15, 5)
# print(y_train[perm_array[0:batch_size]].shape) # (60, 5)

# 定義神經網路變量
# tf.reset_default_graph() 

X = tf.placeholder(tf.float32, [None, time_step, input_size]) # 每批次輸入網路的tensor
Y = tf.placeholder(tf.float32, [None, output_size]) # 每批次輸出網路的tensor

#輸入層、輸出層權重、偏置
weights={
         'in':tf.Variable(tf.random_normal([input_size,rnn_unit]) ,name='w_in'),
         'out':tf.Variable(tf.random_normal([rnn_unit, output_size]) ,name='w_out')
         }
biases={
        'in':tf.Variable(tf.constant(0.1,shape=[rnn_unit, ]) ,name='b_in'),
        'out':tf.Variable(tf.constant(0.1,shape=[output_size, ]) ,name='b_out')
        }

# print(rnn_unit)

# 原本
# X：(batch_size, time_step, input_size)
# def lstm_cell():
#         cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_unit, reuse=tf.global_variables())
#         return tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.8)

def lstm(X, weights, biases):
    global rnn_unit
    w_in=weights['in']
    b_in=biases['in']
    w_out=weights['out']
    b_out=biases['out']

    # ----------input layer----------
    X = tf.reshape(X, [-1, input_size]) # 為了和w_in做矩陣乘法，所以要reshape=>(batch_size*time_step, input_size)
    input_rnn = tf.matmul(X, w_in) + b_in # (batch_size*time_step, rnn_unit)
    input_rnn=tf.nn.relu(input_rnn)
    print('input',input)
    print('input_rnn', input_rnn)
    input_rnn = tf.nn.dropout(input_rnn, keep_prob=0.5) # 保留0.5
    input_rnn = tf.reshape(input_rnn, [-1, time_step, rnn_unit]) # 作為LSTM的input，(batch_size, time_step, rnn_unit)

    # -------------cell-------------
    ## sol 1
    # cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit, state_is_tuple=True, reuse=tf.AUTO_REUSE)
    cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit, forget_bias=0.8, state_is_tuple=True, reuse=tf.AUTO_REUSE)
    mlstm_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([cell]*layers, state_is_tuple = True)  #建立多層的LSTM
    ## sol2
    # mlstm_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(layers)], state_is_tuple = True)
    
    # init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)
    # output_rnn是最後一層每個time_step的輸出；[batch_size, time_step, rnn_unit]
    # final_states每一層的最後一個time_step的輸出；
    # output_rnn, final_states = tf.nn.dynamic_rnn(mlstm_cell, input_rnn, initial_state=init_state, dtype=tf.float32) # 展開
    output_rnn, final_states = tf.nn.dynamic_rnn(mlstm_cell, input_rnn, dtype=tf.float32) # 展開

    # ----------output layer----------
    # # 助教
    # output = tf.reshape(output_rnn, [-1, rnn_unit]) # 作為輸出層的輸入，[batch_size*time_step, rnn_unit]
    # pred = tf.matmul(output, w_out) + b_out # shape：(batch size*time_step, output_size)
    pred = tf.matmul(final_states[1], w_out) + b_out # 莫

    # print('output_rnn: ',output_rnn)
    # print('input rnn: ', input_rnn)
    # print('w_in: ',w_in)
    print('pred.shape: ', pred.shape)

    return pred

int(train_size/batch_size)
print(train_size)

from google.colab import drive
drive.mount('/content/gdrive')

# 訓練model
def train_lstm():
    global batch_size, train_size
    pred = lstm(X, weights, biases)
    print('pred: ',pred)
    loss = tf.reduce_mean(tf.square(pred-Y)) # loss function: MSE # Kaggle
    # loss = tf.reduce_mean(tf.square(pred[:,time_step-1,:]-Y[[:, 3]])) # loss function: MSE # Kaggle
    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # 莫
    # loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1]))) # 助教
    train_op = tf.train.AdamOptimizer(lr).minimize(loss)

    saver=tf.train.Saver(tf.global_variables()) # 儲存模型

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        for i in range(epochs): # 跑幾次epoch，助教原設定50
          step=0
          start=0
          end=start+batch_size
          # 一epoch跑step次
          while(end<train_size):
              x_batch, y_batch = get_next_batch(batch_size) # 取得下個training batch 
              sess.run([train_op], feed_dict={X: x_batch, Y: y_batch})
              start+=batch_size
              end=start+batch_size
              #每50步保存一次參數
              if step%1==0:
                print(i, step, sess.run(loss, feed_dict={X: x_batch, Y: y_batch}))
                # print("保存模型：",saver.save(sess,'/content/gdrive/My Drive/Colab Notebooks/HW2_model_0606_e=1200.ckpt'))
                print("保存模型：",saver.save(sess,r'C:/Users/a9272/OneDrive/桌面/電商作業2/LSTM/HW2_model'))
              step+=1

with tf.variable_scope('train',reuse=tf.AUTO_REUSE):
    train_lstm()

# 預測模型
def prediction():
    pred1 = lstm(X, weights, biases)
    saver=tf.train.Saver(tf.global_variables())
    # global test_size, time_step, weights, biases

    with tf.Session() as sess:
        # 參數恢復
        # module_file = tf.train.latest_checkpoint('/content/gdrive/My Drive/Colab Notebooks/')
        module_file = tf.train.latest_checkpoint(r'C:\Users\a9272\OneDrive\桌面\電商作業2\LSTM')
        saver.restore(sess, module_file) 
        # print(test_x) # 助教：(30,1,1) 我：(30,15,1)
        test_predict=[] # 只放預測的close值
        
        #得到預測結果
        for i in range(30): # test_size = 45
            # prob=sess.run(pred1, feed_dict={X:x_test[i, np.newaxis]}) # 昨天這樣成功，但是出來45個值
            prob=sess.run(pred1, feed_dict={X:x_test[i:i+time_step]}) # 給15天
            # probk的shape(2,15,5)，代表短(1,:,:)、長(2,:,:)期記憶中的每個time_step所預測的每個attribute的值；
            # 真正要的預測結果是：用第time_step天所預測得到的長期記憶，prob[1, time_step-1]，這是預測的值，但我們只要看close，所以取他所在的column「3」
            # -----------我自己加的
            # if i==29:
            #   print(prob.shape)
            #   print(prob)
            prob = prob[1, time_step-1, 0] # 1: LSTM長期記憶、3:close值所在column
            # prob_close =  prob[3] # close在column=3
            # print('i=', i,', close prob=', prob_close)
            print('i=', i,', close prob=', prob)
            # (1)標準化還原
            # prob_close=prob_close.reshape((-1,1)) # 改二維
            # test_predict.extend(prob_close) # 在列表test_predict中加上列表predict
            # (2)不標準化
            # test_predict.append(prob_close) # 在列表test_predict中加上列表predict
            test_predict.append(prob) # 在列表test_predict中加上列表predict
        print(test_predict)

        # # 畫圖
         # (1)標準化還原
        # test_real = y_test[15: , 3].reshape(-1,1) # 最後30天真正的close值
        # (2)不標準化
        # test_real = y_test[15: , 3] # 最後30天真正的close值
        test_real = y_test[test_size-30:, :].reshape(-1) # 最後30天真正的close值 # 轉成一維陣列
        
        global close_scaler
        
        # test_real = close_scaler.inverse_transform(test_real)
        # test_predict = close_scaler.inverse_transform(test_predict)

        # print(test_real)
        # print(test_predict)

        plt.figure()
        plt.plot(list(range(len(test_predict))), test_predict , color='r')
        plt.plot(list(range(len(test_real))), test_real,  color='b')
        plt.show()
        
        # len(test_y)：45
        acc = np.sum(np.abs(list(test_real[i] - test_predict[i] for i in range(len(test_real)))))/len(test_real)
        print(acc)
   

        c = {"real" : test_real,"predict" : test_predict}
        test = pd.DataFrame(c)
        #test=test.T
        print(test)

# x_test[0, np.newaxis].shape # (1, 15, 5)
# print(y_test)
# print(y_test[15:,3]) # 最後30天的close值
# y_test[15:,3] # 最後30天的close值(array)

with tf.variable_scope('train',reuse=True):
    prediction()

# print(y_test[15:].reshape(-1)) # 最後30天真正的值
# y_test[15:].reshape(-1) # 最後30天真正的值

# print(y_test[15: , 3].reshape(-1,1)) # 最後30天真正的值

